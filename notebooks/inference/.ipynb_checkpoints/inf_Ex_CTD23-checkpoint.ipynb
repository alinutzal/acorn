{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/u2/a/alazar/acorn/acorn/core/__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/a/alazar/.conda/envs/acorn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:FRNN is available\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import acorn.core\n",
    "print(acorn.core.__file__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "import yaml\n",
    "from itertools import chain, product, combinations\n",
    "import torch\n",
    "\n",
    "from time import time as tt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from acorn.stages.data_reading import AthenaReader\n",
    "from acorn.core.infer_stage import infer\n",
    "from acorn.core.eval_stage import evaluate\n",
    "\n",
    "from acorn.stages.data_reading.models.trackml_utils import *\n",
    "\n",
    "from acorn.stages.data_reading.data_reading_stage import EventReader\n",
    "from acorn.stages.data_reading.models.trackml_reader import TrackMLReader\n",
    "\n",
    "from acorn.stages.graph_construction.models.metric_learning import MetricLearning\n",
    "from acorn.stages.edge_classifier.models.filter import Filter\n",
    "from acorn.stages.edge_classifier.models.filter import GNNFilter\n",
    "from acorn.stages.edge_classifier import InteractionGNN\n",
    "from acorn.stages.edge_classifier.edge_classifier_stage import EdgeClassifierStage\n",
    "\n",
    "from acorn.stages.graph_construction.utils import handle_weighting\n",
    "from acorn.stages.graph_construction.models.utils import graph_intersection, build_edges\n",
    "from acorn.stages.graph_construction.utils import *\n",
    "from acorn.stages.graph_construction.models.py_module_map import PyModuleMap\n",
    "from acorn.stages.graph_construction.graph_construction_stage import GraphConstructionStage\n",
    "\n",
    "from acorn.stages.track_building import utils \n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "\n",
    "#run = wandb.init(project=model_gnn.hparams[\"project\"], entity='gnnproject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidr = \"../../examples/CTD_2023/data_reader.yaml\"\n",
    "configmm = \"../../examples/CTD_2023/module_map_infer.yaml\"\n",
    "configmm_ev = \"../../examples/CTD_2023/module_map_eval.yaml\"\n",
    "configml = \"../../examples/CTD_2023/metric_learning_infer.yaml\"\n",
    "configml_ev = \"../../examples/CTD_2023/metric_learning_eval.yaml\"\n",
    "configfil = \"../../examples/CTD_2023/filter_infer.yaml\"\n",
    "configfil_ev = \"../../examples/CTD_2023/filter_eval.yaml\"\n",
    "configGnn = \"../../examples/CTD_2023/gnn_infer.yaml\"\n",
    "configGnn_eval = \"../../examples/CTD_2023/gnn_eval.yaml\"\n",
    "configTbi = \"../../examples/CTD_2023/track_building_infer.yaml\"\n",
    "configTbe = \"../../examples/CTD_2023/track_building_eval.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:TrackBuilding:Using log level INFO\n",
      "INFO:TrackBuilding:Using CCandWalk method to reconstruct the tracks\n",
      "Reconstructing tracks for trainset data:   0%|          | 0/1 [00:28<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NetworkXUnfeasible",
     "evalue": "Graph contains a cycle or graph changed during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNetworkXUnfeasible\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#infer(configml)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#evaluate(configml_ev,dataset='testset')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#infer(configfil, checkpoint='/pscratch/sd/a/alazar/cf/CTD_2023/filter/artifacts/best-21725764-auc=0.991198-epoch=42.ckpt')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#evaluate(configfil_ev,dataset='testset')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#infer(configGnn)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#evaluate(configGnn_eval,dataset='testset')\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigTbi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#evaluate(configTbe,None)\u001b[39;00m\n",
      "File \u001b[0;32m/global/u2/a/alazar/acorn/acorn/core/infer_stage.py:71\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(config_file, verbose, checkpoint)\u001b[0m\n\u001b[1;32m     69\u001b[0m     lightning_infer(config, stage_module, checkpoint)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mstage_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/u2/a/alazar/acorn/acorn/stages/track_building/track_building_stage.py:122\u001b[0m, in \u001b[0;36mTrackBuildingStage.infer\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtestset\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(graph_constructor, data_name):\n\u001b[0;32m--> 122\u001b[0m         \u001b[43mgraph_constructor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_tracks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgraph_constructor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_name\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/u2/a/alazar/acorn/acorn/stages/track_building/models/cc_and_walk.py:68\u001b[0m, in \u001b[0;36mCCandWalk.build_tracks\u001b[0;34m(self, dataset, data_name)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     66\u001b[0m         dataset, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReconstructing tracks for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m     ):\n\u001b[0;32m---> 68\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tracks_one_evt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/u2/a/alazar/acorn/acorn/stages/track_building/models/cc_and_walk.py:92\u001b[0m, in \u001b[0;36mCCandWalk._build_tracks_one_evt\u001b[0;34m(self, graph, output_dir)\u001b[0m\n\u001b[1;32m     90\u001b[0m G \u001b[38;5;241m=\u001b[39m cc_and_walk_utils\u001b[38;5;241m.\u001b[39mfilter_graph(graph, score_name, threshold)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# topological sort is really needed only if we run the wrangler afterwards\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[43mcc_and_walk_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopological_sort_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Simple paths from connected components\u001b[39;00m\n\u001b[1;32m     95\u001b[0m all_trks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m cc_and_walk_utils\u001b[38;5;241m.\u001b[39mget_simple_path(G)\n",
      "File \u001b[0;32m/global/u2/a/alazar/acorn/acorn/stages/track_building/models/cc_and_walk_utils.py:65\u001b[0m, in \u001b[0;36mtopological_sort_graph\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m     62\u001b[0m H \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mDiGraph()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Add nodes w/o any features attached\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# maybe this is not needed given line 48?\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[43mH\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_nodes_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopological_sort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# put it after the add nodes\u001b[39;00m\n\u001b[1;32m     68\u001b[0m H\u001b[38;5;241m.\u001b[39madd_edges_from(G\u001b[38;5;241m.\u001b[39medges(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/.conda/envs/acorn/lib/python3.10/site-packages/networkx/classes/digraph.py:530\u001b[0m, in \u001b[0;36mDiGraph.add_nodes_from\u001b[0;34m(self, nodes_for_adding, **attr)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_nodes_from\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes_for_adding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattr):\n\u001b[1;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add multiple nodes.\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m    >>> G.add_nodes_from(list(n + 1 for n in G.nodes))\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes_for_adding:\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m             newnode \u001b[38;5;241m=\u001b[39m n \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node\n",
      "File \u001b[0;32m~/.conda/envs/acorn/lib/python3.10/site-packages/networkx/algorithms/dag.py:309\u001b[0m, in \u001b[0;36mtopological_sort\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;129m@nx\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtopological_sort\u001b[39m(G):\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a generator of nodes in topologically sorted order.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    A topological sort is a nonunique permutation of the nodes of a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m       *Introduction to Algorithms - A Creative Approach.* Addison-Wesley.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mtopological_generations(G):\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m generation\n",
      "File \u001b[0;32m~/.conda/envs/acorn/lib/python3.10/site-packages/networkx/algorithms/dag.py:239\u001b[0m, in \u001b[0;36mtopological_generations\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m this_generation\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indegree_map:\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXUnfeasible(\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph contains a cycle or graph changed during iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m     )\n",
      "\u001b[0;31mNetworkXUnfeasible\u001b[0m: Graph contains a cycle or graph changed during iteration"
     ]
    }
   ],
   "source": [
    "#infer(configmm)\n",
    "#evaluate(configmm_ev,dataset='testset')\n",
    "#infer(configml)\n",
    "#evaluate(configml_ev,dataset='testset')\n",
    "#infer(configfil, checkpoint='/pscratch/sd/a/alazar/cf/CTD_2023/filter/artifacts/best-21725764-auc=0.991198-epoch=42.ckpt')\n",
    "#evaluate(configfil_ev,dataset='testset')\n",
    "#infer(configGnn)\n",
    "#evaluate(configGnn_eval,dataset='testset')\n",
    "infer(configTbi)\n",
    "#evaluate(configTbe,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNFilter(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n",
       "    (2): SiLU()\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "    (5): SiLU()\n",
       "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (7): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
       "    (8): SiLU()\n",
       "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (10): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
       "    (11): SiLU()\n",
       "    (12): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (13): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
       "    (14): SiLU()\n",
       "    (15): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (16): LayerNorm((32,), eps=1e-05, elementwise_affine=False)\n",
       "    (17): SiLU()\n",
       "    (18): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (gnn): GCNEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): SAGEConv(37, 256, aggr=mean)\n",
       "      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): ReLU()\n",
       "      (3): SAGEConv(256, 512, aggr=mean)\n",
       "      (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (5): ReLU()\n",
       "      (6): SAGEConv(512, 1024, aggr=mean)\n",
       "      (7): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (8): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../../examples/CTD_2023/filter_SAGEConv_retrain.yaml\", \"r\") as f:\n",
    "    config_mm = yaml.load(f, Loader=yaml.FullLoader)\n",
    "model_mm = GNNFilter(config_mm)\n",
    "model_mm\n",
    "#model_mm.setup(stage=\"predict\")\n",
    "#model_mm.load_module_map()\n",
    "#model_mm.load_data(\"/scratch/cf/CTD_2023/feature_store/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'nb_node_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../examples/CTD_2023/gnn_train.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     config_gnn \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[0;32m----> 3\u001b[0m model_gnn \u001b[38;5;241m=\u001b[39m \u001b[43mInteractionGNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_gnn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model_gnn\u001b[38;5;241m.\u001b[39msetup(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m model_gnn \u001b[38;5;241m=\u001b[39m InteractionGNN\u001b[38;5;241m.\u001b[39mload_from_checkpoint(config_gnn[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstage_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martifacts/best-v3.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n",
      "File \u001b[0;32m/global/u2/a/alazar/acorn/acorn/stages/edge_classifier/models/interaction_gnn.py:73\u001b[0m, in \u001b[0;36mInteractionGNN.__init__\u001b[0;34m(self, hparams)\u001b[0m\n\u001b[1;32m     68\u001b[0m hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_running_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_running_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Setup input network\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_encoder \u001b[38;5;241m=\u001b[39m make_mlp(\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mlen\u001b[39m(hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_features\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m---> 73\u001b[0m     [hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m*\u001b[39m \u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnb_node_layer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     74\u001b[0m     output_activation\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_activation\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     75\u001b[0m     hidden_activation\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_activation\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     76\u001b[0m     layer_norm\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayernorm\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     77\u001b[0m     batch_norm\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatchnorm\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     78\u001b[0m     track_running_stats\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_running_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# The edge network computes new edge features from connected nodes\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_encoder \u001b[38;5;241m=\u001b[39m make_mlp(\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     84\u001b[0m     [hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m*\u001b[39m hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnb_edge_layer\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m     track_running_stats\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_running_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     90\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'nb_node_layer'"
     ]
    }
   ],
   "source": [
    "with open(\"../../examples/CTD_2023/gnn_train.yaml\", \"r\") as f:\n",
    "    config_gnn = yaml.load(f, Loader=yaml.FullLoader)\n",
    "model_gnn = InteractionGNN(config_gnn)\n",
    "model_gnn.setup('predict')\n",
    "model_gnn = InteractionGNN.load_from_checkpoint(config_gnn['stage_dir']+'artifacts/best-v3.ckpt')  \n",
    "                                                #best-4l0jlwuh-val_loss=0.085163-epoch=77.ckpt')\n",
    "#dataloaders_gnn = model_gnn.predict_dataloader()\n",
    "\n",
    "config_tbe = yaml.safe_load(open(\"../../examples/CTD_2023/track_building_eval.yaml\", \"r\"))\n",
    "print(config_gnn['stage_dir']+'artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labelled_graphs(graphset, config):\n",
    "    all_y_truth, all_pt  = [], []\n",
    "    evaluated_events = [\n",
    "        utils.evaluate_labelled_graph(\n",
    "            event,\n",
    "            sel_conf=config[\"target_tracks\"],\n",
    "            matching_fraction=config[\"matching_fraction\"],\n",
    "            matching_style=config[\"matching_style\"],\n",
    "            min_track_length=config[\"min_track_length\"],\n",
    "        )\n",
    "        for event in tqdm(graphset)\n",
    "    ]\n",
    "    evaluated_events = pd.concat(evaluated_events)\n",
    "\n",
    "    particles = evaluated_events[evaluated_events[\"is_reconstructable\"]]\n",
    "    reconstructed_particles = particles[particles[\"is_reconstructed\"] & particles[\"is_matchable\"]]\n",
    "    tracks = evaluated_events[evaluated_events[\"is_matchable\"]]\n",
    "    matched_tracks = tracks[tracks[\"is_matched\"]]\n",
    "\n",
    "    n_particles = len(particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "    n_reconstructed_particles = len(reconstructed_particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "\n",
    "    n_tracks = len(tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "    n_matched_tracks = len(matched_tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "\n",
    "    n_dup_reconstructed_particles = len(reconstructed_particles) - n_reconstructed_particles\n",
    "\n",
    "    print(f\"Number of reconstructed particles: {n_reconstructed_particles}\")\n",
    "    print(f\"Number of particles: {n_particles}\")\n",
    "    print(f\"Number of matched tracks: {n_matched_tracks}\")\n",
    "    print(f\"Number of tracks: {n_tracks}\")\n",
    "    print(f\"Number of duplicate reconstructed particles: {n_dup_reconstructed_particles}\")   \n",
    "\n",
    "    # Plot the results across pT and eta\n",
    "    eff = n_reconstructed_particles / n_particles\n",
    "    fake_rate = 1 - (n_matched_tracks / n_tracks)\n",
    "    dup_rate = n_dup_reconstructed_particles / n_reconstructed_particles\n",
    "\n",
    "    logging.info(f\"Efficiency: {eff:.3f}\")\n",
    "    logging.info(f\"Fake rate: {fake_rate:.3f}\")\n",
    "    logging.info(f\"Duplication rate: {dup_rate:.3f}\")\n",
    "    print(f\"Efficiency: {eff:.3f}\")\n",
    "    print(f\"Fake rate: {fake_rate:.3f}\")\n",
    "    print(f\"Duplication rate: {dup_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_result_summary(\n",
    "    n_reconstructed_particles,\n",
    "    n_particles,\n",
    "    n_matched_tracks,\n",
    "    n_tracks,\n",
    "    n_dup_reconstructed_particles,\n",
    "    eff,\n",
    "    fake_rate,\n",
    "    dup_rate,\n",
    "):\n",
    "    summary = f\"Number of reconstructed particles: {n_reconstructed_particles}\\n\"\n",
    "    summary += f\"Number of particles: {n_particles}\\n\"\n",
    "    summary += f\"Number of matched tracks: {n_matched_tracks}\\n\"\n",
    "    summary += f\"Number of tracks: {n_tracks}\\n\"\n",
    "    summary += (\n",
    "        \"Number of duplicate reconstructed particles:\"\n",
    "        f\" {n_dup_reconstructed_particles}\\n\"\n",
    "    )\n",
    "    summary += f\"Efficiency: {eff:.3f}\\n\"\n",
    "    summary += f\"Fake rate: {fake_rate:.3f}\\n\"\n",
    "    summary += f\"Duplication rate: {dup_rate:.3f}\\n\"\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def tracking_efficiency(dataset, config): #plot_config,\n",
    "    \"\"\"\n",
    "    Plot the track efficiency vs. pT of the edge.\n",
    "    \"\"\"\n",
    "    all_y_truth, all_pt = [], []\n",
    "    #dataset = getattr(self, config[\"dataset\"])\n",
    "\n",
    "    evaluated_events = []\n",
    "    for event in tqdm(dataset):\n",
    "        evaluated_events.append(\n",
    "            utils.evaluate_labelled_graph(\n",
    "                event,\n",
    "                matching_fraction=config[\"matching_fraction\"],\n",
    "                matching_style=config[\"matching_style\"],\n",
    "                sel_conf=config[\"target_tracks\"],\n",
    "                min_track_length=config[\"min_track_length\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    evaluated_events = pd.concat(evaluated_events)\n",
    "\n",
    "    particles = evaluated_events[evaluated_events[\"is_reconstructable\"]]\n",
    "    reconstructed_particles = particles[\n",
    "        particles[\"is_reconstructed\"] & particles[\"is_matchable\"]\n",
    "    ]\n",
    "    tracks = evaluated_events[evaluated_events[\"is_matchable\"]]\n",
    "    matched_tracks = tracks[tracks[\"is_matched\"]]\n",
    "\n",
    "    n_particles = len(particles.drop_duplicates(subset=[\"event_id\", \"particle_id\"]))\n",
    "    n_reconstructed_particles = len(\n",
    "        reconstructed_particles.drop_duplicates(subset=[\"event_id\", \"particle_id\"])\n",
    "    )\n",
    "\n",
    "    n_tracks = len(tracks.drop_duplicates(subset=[\"event_id\", \"track_id\"]))\n",
    "    n_matched_tracks = len(\n",
    "        matched_tracks.drop_duplicates(subset=[\"event_id\", \"track_id\"])\n",
    "    )\n",
    "\n",
    "    n_dup_reconstructed_particles = (\n",
    "        len(reconstructed_particles) - n_reconstructed_particles\n",
    "    )\n",
    "\n",
    "    eff = n_reconstructed_particles / n_particles\n",
    "    fake_rate = 1 - (n_matched_tracks / n_tracks)\n",
    "    dup_rate = n_dup_reconstructed_particles / n_reconstructed_particles\n",
    "\n",
    "    result_summary = make_result_summary(\n",
    "        n_reconstructed_particles,\n",
    "        n_particles,\n",
    "        n_matched_tracks,\n",
    "        n_tracks,\n",
    "        n_dup_reconstructed_particles,\n",
    "        eff,\n",
    "        fake_rate,\n",
    "        dup_rate,\n",
    "    )\n",
    "\n",
    "    print(f\"Number of reconstructed particles: {n_reconstructed_particles}\")\n",
    "    print(f\"Number of particles: {n_particles}\")\n",
    "    print(f\"Number of matched tracks: {n_matched_tracks}\")\n",
    "    print(f\"Number of tracks: {n_tracks}\")\n",
    "    print(f\"Number of duplicate reconstructed particles: {n_dup_reconstructed_particles}\")   \n",
    "    print(f\"Efficiency: {eff:.3f}\")\n",
    "    print(f\"Fake rate: {fake_rate:.3f}\")\n",
    "    print(f\"Duplication rate: {dup_rate:.3f}\")\n",
    "\n",
    "    #self.log.info(\"Result Summary :\\n\\n\" + result_summary)\n",
    "\n",
    "    # res_fname = os.path.join(\n",
    "    #     self.hparams[\"stage_dir\"],\n",
    "    #     f\"results_summary_{self.hparams['matching_style']}.txt\",\n",
    "    # )\n",
    "\n",
    "    # with open(res_fname, \"w\") as f:\n",
    "    #     f.write(result_summary)\n",
    "\n",
    "    # First get the list of particles without duplicates\n",
    "    grouped_reco_particles = particles.groupby(\"particle_id\")[\n",
    "        \"is_reconstructed\"\n",
    "    ].any()\n",
    "    # particles[\"is_reconstructed\"] = particles[\"particle_id\"].isin(grouped_reco_particles[grouped_reco_particles].index.values)\n",
    "    particles.loc[\n",
    "        particles[\"particle_id\"].isin(\n",
    "            grouped_reco_particles[grouped_reco_particles].index.values\n",
    "        ),\n",
    "        \"is_reconstructed\",\n",
    "    ] = True\n",
    "    particles = particles.drop_duplicates(subset=[\"particle_id\"])\n",
    "\n",
    "    # Plot the results across pT and eta (if provided in conf file)\n",
    "    #os.makedirs(self.hparams[\"stage_dir\"], exist_ok=True)\n",
    "\n",
    "    # for var, varconf in plot_config[\"variables\"].items():\n",
    "    #     utils.plot_eff(\n",
    "    #         particles,\n",
    "    #         var,\n",
    "    #         varconf,\n",
    "    #         save_path=os.path.join(\n",
    "    #             self.hparams[\"stage_dir\"],\n",
    "    #             f\"track_reconstruction_eff_vs_{var}_{self.hparams['matching_style']}.png\",\n",
    "    #         ),\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reconstructed particles: 1263\n",
      "Number of particles: 2277\n",
      "Number of matched tracks: 29530\n",
      "Number of tracks: 41903\n",
      "Number of duplicate reconstructed particles: 813\n",
      "Efficiency: 0.555\n",
      "Fake rate: 0.295\n",
      "Duplication rate: 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device ='cuda'\n",
    "model_mm = model_mm.to(\"cuda\")\n",
    "model_gnn = model_gnn.to(\"cuda\")\n",
    "graphs = []\n",
    "for batch_idx, (graph, _, truth) in enumerate(model_mm.testset):\n",
    "    print(batch_idx)\n",
    "    batch = model_mm.build_graph(graph.to(device), truth).to(device)\n",
    "    batch.weights = handle_weighting(batch, model_gnn.hparams[\"weighting\"]) #torch.ones_like(batch.y, dtype=torch.float32)\n",
    "    gnn = model_gnn.shared_evaluation(batch,batch_idx)\n",
    "    batch = gnn['batch']\n",
    "    # with torch.no_grad():\n",
    "    #     if device == 'cuda':\n",
    "    #         with torch.cuda.amp.autocast():\n",
    "    #             out = model_gnn(batch)\n",
    "    # batch.scores = torch.sigmoid(out)\n",
    "    #model_gnn.log_metrics(gnn['output'],gnn['all_truth'],gnn['target_truth'],gnn['loss'])\n",
    "    edge_mask = gnn['output'] > 0.8 #model_gnn.hparams['edge_cut'] # score_cut for evaluation\n",
    "    \n",
    "    # Get number of nodes\n",
    "    if hasattr(batch, \"num_nodes\"):\n",
    "        num_nodes = batch.num_nodes\n",
    "    elif hasattr(batch, \"x\"):\n",
    "        num_nodes = batch.x.size(0)\n",
    "    elif hasattr(batch, \"x_x\"):\n",
    "        num_nodes = batch.x_x.size(0)\n",
    "    else:\n",
    "        num_nodes = batch.edge_index.max().item() + 1\n",
    "    # Convert to sparse scipy array\n",
    "    sparse_edges = to_scipy_sparse_matrix(\n",
    "        batch.edge_index[:, edge_mask], num_nodes=num_nodes\n",
    "    )\n",
    "    # Run connected components\n",
    "    candidate_labels = sps.csgraph.connected_components(\n",
    "        sparse_edges, directed=False, return_labels=True\n",
    "    )\n",
    "    batch.labels = torch.from_numpy(candidate_labels[1]).long()\n",
    "    graphs.append(batch.to('cpu'))\n",
    "\n",
    "#evaluate_labelled_graphs(graphs, config_tbe)\n",
    "tracking_efficiency(graphs, config_tbe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acorn",
   "language": "python",
   "name": "acorn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
