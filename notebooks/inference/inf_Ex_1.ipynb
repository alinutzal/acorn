{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/homes/a/alazar/acorn_new/acorn/core/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import acorn.core\n",
    "print(acorn.core.__file__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "import yaml\n",
    "from itertools import chain, product, combinations\n",
    "import torch\n",
    "\n",
    "from time import time as tt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from acorn.stages.data_reading import AthenaReader\n",
    "from acorn.core.infer_stage import infer\n",
    "from acorn.core.eval_stage import evaluate\n",
    "\n",
    "from acorn.stages.data_reading.models.trackml_utils import *\n",
    "\n",
    "from acorn.stages.data_reading.data_reading_stage import EventReader\n",
    "from acorn.stages.data_reading.models.trackml_reader import TrackMLReader\n",
    "\n",
    "from acorn.stages.graph_construction.models.metric_learning import MetricLearning\n",
    "from acorn.stages.edge_classifier.models.filter import Filter\n",
    "from acorn.stages.edge_classifier import InteractionGNN\n",
    "from acorn.stages.edge_classifier.edge_classifier_stage import EdgeClassifierStage\n",
    "\n",
    "from acorn.stages.graph_construction.utils import handle_weighting\n",
    "from acorn.stages.graph_construction.models.utils import graph_intersection, build_edges\n",
    "from acorn.stages.graph_construction.utils import *\n",
    "from acorn.stages.graph_construction.models.py_module_map import PyModuleMap\n",
    "from acorn.stages.graph_construction.graph_construction_stage import GraphConstructionStage\n",
    "\n",
    "from acorn.stages.track_building import utils \n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "\n",
    "#run = wandb.init(project=model_gnn.hparams[\"project\"], entity='gnnproject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidr = \"../../examples/Example_1/data_reader.yaml\"\n",
    "configmm = \"../../examples/Example_1/module_map_infer.yaml\"\n",
    "configGnn = \"../../examples/Example_1/gnn_infer.yaml\"\n",
    "configGnn_eval = \"../../examples/Example_1/gnn_eval.yaml\"\n",
    "configTbi = \"../../examples/Example_1/track_building_infer.yaml\"\n",
    "configTbe = \"../../examples/Example_1/track_building_eval.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate(configTbe,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 80 training events, 10 validation events and 2 testing events\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../examples/Example_1/module_map_infer.yaml\", \"r\") as f:\n",
    "    config_mm = yaml.load(f, Loader=yaml.FullLoader)\n",
    "model_mm = PyModuleMap(config_mm)\n",
    "#model_mm.setup(stage=\"predict\")\n",
    "model_mm.load_module_map()\n",
    "model_mm.load_data(\"/scratch/cf/Example_1/feature_store/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [particle_id] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [nhits] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [primary] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [pdgId] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [ghost] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [shared] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [module_id] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [region] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [hit_id] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [pt] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/stages/edge_classifier/edge_classifier_stage.py:96: UserWarning: Failed to define figures of merit, due to logger unavailable\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining figures of merit\n",
      "/scratch/cf/Example_1/gnn/artifacts\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../examples/Example_1/gnn_train.yaml\", \"r\") as f:\n",
    "    config_gnn = yaml.load(f, Loader=yaml.FullLoader)\n",
    "model_gnn = InteractionGNN(config_gnn)\n",
    "model_gnn.setup('predict')\n",
    "model_gnn = InteractionGNN.load_from_checkpoint(config_gnn['stage_dir']+'artifacts/best-v3.ckpt')  \n",
    "                                                #best-4l0jlwuh-val_loss=0.085163-epoch=77.ckpt')\n",
    "#dataloaders_gnn = model_gnn.predict_dataloader()\n",
    "\n",
    "config_tbe = yaml.safe_load(open(\"../../examples/Example_1/track_building_eval.yaml\", \"r\"))\n",
    "print(config_gnn['stage_dir']+'artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labelled_graphs(graphset, config):\n",
    "    all_y_truth, all_pt  = [], []\n",
    "    evaluated_events = [\n",
    "        utils.evaluate_labelled_graph(\n",
    "            event,\n",
    "            sel_conf=config[\"target_tracks\"],\n",
    "            matching_fraction=config[\"matching_fraction\"],\n",
    "            matching_style=config[\"matching_style\"],\n",
    "            min_track_length=config[\"min_track_length\"],\n",
    "        )\n",
    "        for event in tqdm(graphset)\n",
    "    ]\n",
    "    evaluated_events = pd.concat(evaluated_events)\n",
    "\n",
    "    particles = evaluated_events[evaluated_events[\"is_reconstructable\"]]\n",
    "    reconstructed_particles = particles[particles[\"is_reconstructed\"] & particles[\"is_matchable\"]]\n",
    "    tracks = evaluated_events[evaluated_events[\"is_matchable\"]]\n",
    "    matched_tracks = tracks[tracks[\"is_matched\"]]\n",
    "\n",
    "    n_particles = len(particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "    n_reconstructed_particles = len(reconstructed_particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "\n",
    "    n_tracks = len(tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "    n_matched_tracks = len(matched_tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "\n",
    "    n_dup_reconstructed_particles = len(reconstructed_particles) - n_reconstructed_particles\n",
    "\n",
    "    print(f\"Number of reconstructed particles: {n_reconstructed_particles}\")\n",
    "    print(f\"Number of particles: {n_particles}\")\n",
    "    print(f\"Number of matched tracks: {n_matched_tracks}\")\n",
    "    print(f\"Number of tracks: {n_tracks}\")\n",
    "    print(f\"Number of duplicate reconstructed particles: {n_dup_reconstructed_particles}\")   \n",
    "\n",
    "    # Plot the results across pT and eta\n",
    "    eff = n_reconstructed_particles / n_particles\n",
    "    fake_rate = 1 - (n_matched_tracks / n_tracks)\n",
    "    dup_rate = n_dup_reconstructed_particles / n_reconstructed_particles\n",
    "\n",
    "    logging.info(f\"Efficiency: {eff:.3f}\")\n",
    "    logging.info(f\"Fake rate: {fake_rate:.3f}\")\n",
    "    logging.info(f\"Duplication rate: {dup_rate:.3f}\")\n",
    "    print(f\"Efficiency: {eff:.3f}\")\n",
    "    print(f\"Fake rate: {fake_rate:.3f}\")\n",
    "    print(f\"Duplication rate: {dup_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_result_summary(\n",
    "    n_reconstructed_particles,\n",
    "    n_particles,\n",
    "    n_matched_tracks,\n",
    "    n_tracks,\n",
    "    n_dup_reconstructed_particles,\n",
    "    eff,\n",
    "    fake_rate,\n",
    "    dup_rate,\n",
    "):\n",
    "    summary = f\"Number of reconstructed particles: {n_reconstructed_particles}\\n\"\n",
    "    summary += f\"Number of particles: {n_particles}\\n\"\n",
    "    summary += f\"Number of matched tracks: {n_matched_tracks}\\n\"\n",
    "    summary += f\"Number of tracks: {n_tracks}\\n\"\n",
    "    summary += (\n",
    "        \"Number of duplicate reconstructed particles:\"\n",
    "        f\" {n_dup_reconstructed_particles}\\n\"\n",
    "    )\n",
    "    summary += f\"Efficiency: {eff:.3f}\\n\"\n",
    "    summary += f\"Fake rate: {fake_rate:.3f}\\n\"\n",
    "    summary += f\"Duplication rate: {dup_rate:.3f}\\n\"\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def tracking_efficiency(dataset, config): #plot_config,\n",
    "    \"\"\"\n",
    "    Plot the track efficiency vs. pT of the edge.\n",
    "    \"\"\"\n",
    "    all_y_truth, all_pt = [], []\n",
    "    #dataset = getattr(self, config[\"dataset\"])\n",
    "\n",
    "    evaluated_events = []\n",
    "    for event in tqdm(dataset):\n",
    "        evaluated_events.append(\n",
    "            utils.evaluate_labelled_graph(\n",
    "                event,\n",
    "                matching_fraction=config[\"matching_fraction\"],\n",
    "                matching_style=config[\"matching_style\"],\n",
    "                sel_conf=config[\"target_tracks\"],\n",
    "                min_track_length=config[\"min_track_length\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    evaluated_events = pd.concat(evaluated_events)\n",
    "\n",
    "    particles = evaluated_events[evaluated_events[\"is_reconstructable\"]]\n",
    "    reconstructed_particles = particles[\n",
    "        particles[\"is_reconstructed\"] & particles[\"is_matchable\"]\n",
    "    ]\n",
    "    tracks = evaluated_events[evaluated_events[\"is_matchable\"]]\n",
    "    matched_tracks = tracks[tracks[\"is_matched\"]]\n",
    "\n",
    "    n_particles = len(particles.drop_duplicates(subset=[\"event_id\", \"particle_id\"]))\n",
    "    n_reconstructed_particles = len(\n",
    "        reconstructed_particles.drop_duplicates(subset=[\"event_id\", \"particle_id\"])\n",
    "    )\n",
    "\n",
    "    n_tracks = len(tracks.drop_duplicates(subset=[\"event_id\", \"track_id\"]))\n",
    "    n_matched_tracks = len(\n",
    "        matched_tracks.drop_duplicates(subset=[\"event_id\", \"track_id\"])\n",
    "    )\n",
    "\n",
    "    n_dup_reconstructed_particles = (\n",
    "        len(reconstructed_particles) - n_reconstructed_particles\n",
    "    )\n",
    "\n",
    "    eff = n_reconstructed_particles / n_particles\n",
    "    fake_rate = 1 - (n_matched_tracks / n_tracks)\n",
    "    dup_rate = n_dup_reconstructed_particles / n_reconstructed_particles\n",
    "\n",
    "    result_summary = make_result_summary(\n",
    "        n_reconstructed_particles,\n",
    "        n_particles,\n",
    "        n_matched_tracks,\n",
    "        n_tracks,\n",
    "        n_dup_reconstructed_particles,\n",
    "        eff,\n",
    "        fake_rate,\n",
    "        dup_rate,\n",
    "    )\n",
    "\n",
    "    print(f\"Number of reconstructed particles: {n_reconstructed_particles}\")\n",
    "    print(f\"Number of particles: {n_particles}\")\n",
    "    print(f\"Number of matched tracks: {n_matched_tracks}\")\n",
    "    print(f\"Number of tracks: {n_tracks}\")\n",
    "    print(f\"Number of duplicate reconstructed particles: {n_dup_reconstructed_particles}\")   \n",
    "    print(f\"Efficiency: {eff:.3f}\")\n",
    "    print(f\"Fake rate: {fake_rate:.3f}\")\n",
    "    print(f\"Duplication rate: {dup_rate:.3f}\")\n",
    "\n",
    "    #self.log.info(\"Result Summary :\\n\\n\" + result_summary)\n",
    "\n",
    "    # res_fname = os.path.join(\n",
    "    #     self.hparams[\"stage_dir\"],\n",
    "    #     f\"results_summary_{self.hparams['matching_style']}.txt\",\n",
    "    # )\n",
    "\n",
    "    # with open(res_fname, \"w\") as f:\n",
    "    #     f.write(result_summary)\n",
    "\n",
    "    # First get the list of particles without duplicates\n",
    "    grouped_reco_particles = particles.groupby(\"particle_id\")[\n",
    "        \"is_reconstructed\"\n",
    "    ].any()\n",
    "    # particles[\"is_reconstructed\"] = particles[\"particle_id\"].isin(grouped_reco_particles[grouped_reco_particles].index.values)\n",
    "    particles.loc[\n",
    "        particles[\"particle_id\"].isin(\n",
    "            grouped_reco_particles[grouped_reco_particles].index.values\n",
    "        ),\n",
    "        \"is_reconstructed\",\n",
    "    ] = True\n",
    "    particles = particles.drop_duplicates(subset=[\"particle_id\"])\n",
    "\n",
    "    # Plot the results across pT and eta (if provided in conf file)\n",
    "    #os.makedirs(self.hparams[\"stage_dir\"], exist_ok=True)\n",
    "\n",
    "    # for var, varconf in plot_config[\"variables\"].items():\n",
    "    #     utils.plot_eff(\n",
    "    #         particles,\n",
    "    #         var,\n",
    "    #         varconf,\n",
    "    #         save_path=os.path.join(\n",
    "    #             self.hparams[\"stage_dir\"],\n",
    "    #             f\"track_reconstruction_eff_vs_{var}_{self.hparams['matching_style']}.png\",\n",
    "    #         ),\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  8.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reconstructed particles: 1267\n",
      "Number of particles: 2277\n",
      "Number of matched tracks: 29534\n",
      "Number of tracks: 41898\n",
      "Number of duplicate reconstructed particles: 812\n",
      "Efficiency: 0.556\n",
      "Fake rate: 0.295\n",
      "Duplication rate: 0.641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device ='cuda'\n",
    "model_mm = model_mm.to(\"cuda\")\n",
    "model_gnn = model_gnn.to(\"cuda\")\n",
    "graphs = []\n",
    "for batch_idx, (graph, _, truth) in enumerate(model_mm.testset):\n",
    "    print(batch_idx)\n",
    "    batch = model_mm.build_graph(graph, truth).to(device)\n",
    "    batch.weights = handle_weighting(batch, model_gnn.hparams[\"weighting\"]) #torch.ones_like(batch.y, dtype=torch.float32)\n",
    "    gnn = model_gnn.shared_evaluation(batch,batch_idx)\n",
    "    batch = gnn['batch']\n",
    "    # with torch.no_grad():\n",
    "    #     if device == 'cuda':\n",
    "    #         with torch.cuda.amp.autocast():\n",
    "    #             out = model_gnn(batch)\n",
    "    # batch.scores = torch.sigmoid(out)\n",
    "    #model_gnn.log_metrics(gnn['output'],gnn['all_truth'],gnn['target_truth'],gnn['loss'])\n",
    "    edge_mask = gnn['output'] >= 0.8 #model_gnn.hparams['edge_cut'] # score_cut for evaluation\n",
    "    \n",
    "    # Get number of nodes\n",
    "    if hasattr(batch, \"num_nodes\"):\n",
    "        num_nodes = batch.num_nodes\n",
    "    elif hasattr(batch, \"x\"):\n",
    "        num_nodes = batch.x.size(0)\n",
    "    elif hasattr(batch, \"x_x\"):\n",
    "        num_nodes = batch.x_x.size(0)\n",
    "    else:\n",
    "        num_nodes = batch.edge_index.max().item() + 1\n",
    "    # Convert to sparse scipy array\n",
    "    sparse_edges = to_scipy_sparse_matrix(\n",
    "        batch.edge_index[:, edge_mask], num_nodes=num_nodes\n",
    "    )\n",
    "    # Run connected components\n",
    "    candidate_labels = sps.csgraph.connected_components(\n",
    "        sparse_edges, directed=False, return_labels=True\n",
    "    )\n",
    "    batch.labels = torch.from_numpy(candidate_labels[1]).long()\n",
    "    graphs.append(batch.to('cpu'))\n",
    "\n",
    "#evaluate_labelled_graphs(graphs, config_tbe)\n",
    "tracking_efficiency(graphs, config_tbe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn4itk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
