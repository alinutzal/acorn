{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/u2/a/alazar/acorn/acorn/core/__init__.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/a/alazar/.conda/envs/acorn/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:FRNN is available\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import acorn.core\n",
    "print(acorn.core.__file__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "import yaml\n",
    "from itertools import chain, product, combinations\n",
    "import torch\n",
    "\n",
    "from time import time as tt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from acorn.stages.data_reading import AthenaReader\n",
    "from acorn.core.infer_stage import infer\n",
    "from acorn.core.eval_stage import evaluate\n",
    "\n",
    "from acorn.stages.data_reading.models.trackml_utils import *\n",
    "\n",
    "from acorn.stages.data_reading.data_reading_stage import EventReader\n",
    "from acorn.stages.data_reading.models.trackml_reader import TrackMLReader\n",
    "\n",
    "from acorn.stages.graph_construction.models.metric_learning import MetricLearning\n",
    "from acorn.stages.edge_classifier.models.filter import Filter\n",
    "from acorn.stages.edge_classifier.models.filter import GNNFilter\n",
    "from acorn.stages.edge_classifier import InteractionGNN\n",
    "from acorn.stages.edge_classifier.edge_classifier_stage import EdgeClassifierStage\n",
    "\n",
    "from acorn.stages.graph_construction.utils import handle_weighting\n",
    "from acorn.stages.graph_construction.models.utils import graph_intersection, build_edges\n",
    "from acorn.stages.graph_construction.utils import *\n",
    "from acorn.stages.graph_construction.models.py_module_map import PyModuleMap\n",
    "from acorn.stages.graph_construction.graph_construction_stage import GraphConstructionStage\n",
    "\n",
    "from acorn.stages.track_building import utils \n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "\n",
    "#run = wandb.init(project=model_gnn.hparams[\"project\"], entity='gnnproject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "confidr = \"../../examples/CTD_2023/data_reader.yaml\"\n",
    "configmm = \"../../examples/CTD_2023/module_map_infer.yaml\"\n",
    "configmm_ev = \"../../examples/CTD_2023/module_map_eval.yaml\"\n",
    "configml = \"../../examples/CTD_2023/metric_learning_infer.yaml\"\n",
    "configml_ev = \"../../examples/CTD_2023/metric_learning_eval.yaml\"\n",
    "configfil = \"../../examples/CTD_2023/filter_infer.yaml\"\n",
    "configfil_ev = \"../../examples/CTD_2023/filter_eval.yaml\"\n",
    "configGnn = \"../../examples/CTD_2023/gnn_infer.yaml\"\n",
    "configGnn_eval = \"../../examples/CTD_2023/gnn_eval.yaml\"\n",
    "configTbi = \"../../examples/CTD_2023/track_building_infer.yaml\"\n",
    "configTbe = \"../../examples/CTD_2023/track_building_eval.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: /pscratch/sd/a/alazar/cf/CTD_2023/metric_learning/artifacts/best-11292882-f1=0.006355.ckpt\n",
      "Loaded 1 training events, 1 validation events and 1 testing events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/u2/a/alazar/acorn/acorn/utils/loading_utils.py:79: UserWarning: OPTIONAL feature [pid] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/u2/a/alazar/acorn/acorn/utils/loading_utils.py:79: UserWarning: OPTIONAL feature [n_hits] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/u2/a/alazar/acorn/acorn/utils/loading_utils.py:79: UserWarning: OPTIONAL feature [primary] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/u2/a/alazar/acorn/acorn/utils/loading_utils.py:79: UserWarning: OPTIONAL feature [pdg_id] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/u2/a/alazar/acorn/acorn/utils/loading_utils.py:79: UserWarning: OPTIONAL feature [ghost] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/u2/a/alazar/acorn/acorn/utils/loading_utils.py:79: UserWarning: OPTIONAL feature [shared] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/u2/a/alazar/acorn/acorn/utils/loading_utils.py:79: UserWarning: OPTIONAL feature [module_id] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/u2/a/alazar/acorn/acorn/utils/loading_utils.py:79: UserWarning: OPTIONAL feature [region_id] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/u2/a/alazar/acorn/acorn/utils/loading_utils.py:79: UserWarning: OPTIONAL feature [hit_id] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: IPU available: False, using: 0 IPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 training events, 1 validation events and 1 testing events\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n",
      "/global/homes/a/alazar/.conda/envs/acorn/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:01<00:00,  0.86it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/a/alazar/.conda/envs/acorn/lib/python3.10/site-packages/pytorch_lightning/loops/prediction_loop.py:252: predict returned None if it was on purpose, ignore this warning...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 2: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "#infer(configmm)\n",
    "#evaluate(configmm_ev,dataset='testset')\n",
    "infer(configml)\n",
    "#evaluate(configml_ev,dataset='testset')\n",
    "#infer(configfil, checkpoint='/pscratch/sd/a/alazar/cf/CTD_2023/filter/artifacts/best-21725764-auc=0.991198-epoch=42.ckpt')\n",
    "#evaluate(configfil_ev,dataset='testset')\n",
    "#infer(configGnn)\n",
    "#evaluate(configGnn_eval,dataset='testset')\n",
    "#infer(configTbi)\n",
    "#evaluate(configTbe,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GNNFilter(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=False)\n",
       "    (2): SiLU()\n",
       "    (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (4): LayerNorm((512,), eps=1e-05, elementwise_affine=False)\n",
       "    (5): SiLU()\n",
       "    (6): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (7): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
       "    (8): SiLU()\n",
       "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (10): LayerNorm((128,), eps=1e-05, elementwise_affine=False)\n",
       "    (11): SiLU()\n",
       "    (12): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (13): LayerNorm((64,), eps=1e-05, elementwise_affine=False)\n",
       "    (14): SiLU()\n",
       "    (15): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (16): LayerNorm((32,), eps=1e-05, elementwise_affine=False)\n",
       "    (17): SiLU()\n",
       "    (18): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (gnn): GCNEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): SAGEConv(37, 256, aggr=mean)\n",
       "      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): ReLU()\n",
       "      (3): SAGEConv(256, 512, aggr=mean)\n",
       "      (4): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (5): ReLU()\n",
       "      (6): SAGEConv(512, 1024, aggr=mean)\n",
       "      (7): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (8): ReLU()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../../examples/CTD_2023/filter_SAGEConv_retrain.yaml\", \"r\") as f:\n",
    "    config_mm = yaml.load(f, Loader=yaml.FullLoader)\n",
    "model_mm = GNNFilter(config_mm)\n",
    "model_mm\n",
    "#model_mm.setup(stage=\"predict\")\n",
    "#model_mm.load_module_map()\n",
    "#model_mm.load_data(\"/scratch/cf/CTD_2023/feature_store/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'nb_node_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../examples/CTD_2023/gnn_train.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     config_gnn \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39mload(f, Loader\u001b[38;5;241m=\u001b[39myaml\u001b[38;5;241m.\u001b[39mFullLoader)\n\u001b[0;32m----> 3\u001b[0m model_gnn \u001b[38;5;241m=\u001b[39m \u001b[43mInteractionGNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_gnn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m model_gnn\u001b[38;5;241m.\u001b[39msetup(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m model_gnn \u001b[38;5;241m=\u001b[39m InteractionGNN\u001b[38;5;241m.\u001b[39mload_from_checkpoint(config_gnn[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstage_dir\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martifacts/best-v3.ckpt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n",
      "File \u001b[0;32m/global/u2/a/alazar/acorn/acorn/stages/edge_classifier/models/interaction_gnn.py:73\u001b[0m, in \u001b[0;36mInteractionGNN.__init__\u001b[0;34m(self, hparams)\u001b[0m\n\u001b[1;32m     68\u001b[0m hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_running_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_running_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Setup input network\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_encoder \u001b[38;5;241m=\u001b[39m make_mlp(\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mlen\u001b[39m(hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_features\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[0;32m---> 73\u001b[0m     [hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m*\u001b[39m \u001b[43mhparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnb_node_layer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m     74\u001b[0m     output_activation\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_activation\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     75\u001b[0m     hidden_activation\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_activation\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     76\u001b[0m     layer_norm\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayernorm\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     77\u001b[0m     batch_norm\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatchnorm\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     78\u001b[0m     track_running_stats\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_running_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# The edge network computes new edge features from connected nodes\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_encoder \u001b[38;5;241m=\u001b[39m make_mlp(\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m (hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m\"\u001b[39m]),\n\u001b[1;32m     84\u001b[0m     [hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m*\u001b[39m hparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnb_edge_layer\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m     track_running_stats\u001b[38;5;241m=\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_running_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     90\u001b[0m )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'nb_node_layer'"
     ]
    }
   ],
   "source": [
    "with open(\"../../examples/CTD_2023/gnn_train.yaml\", \"r\") as f:\n",
    "    config_gnn = yaml.load(f, Loader=yaml.FullLoader)\n",
    "model_gnn = InteractionGNN(config_gnn)\n",
    "model_gnn.setup('predict')\n",
    "model_gnn = InteractionGNN.load_from_checkpoint(config_gnn['stage_dir']+'artifacts/best-v3.ckpt')  \n",
    "                                                #best-4l0jlwuh-val_loss=0.085163-epoch=77.ckpt')\n",
    "#dataloaders_gnn = model_gnn.predict_dataloader()\n",
    "\n",
    "config_tbe = yaml.safe_load(open(\"../../examples/CTD_2023/track_building_eval.yaml\", \"r\"))\n",
    "print(config_gnn['stage_dir']+'artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labelled_graphs(graphset, config):\n",
    "    all_y_truth, all_pt  = [], []\n",
    "    evaluated_events = [\n",
    "        utils.evaluate_labelled_graph(\n",
    "            event,\n",
    "            sel_conf=config[\"target_tracks\"],\n",
    "            matching_fraction=config[\"matching_fraction\"],\n",
    "            matching_style=config[\"matching_style\"],\n",
    "            min_track_length=config[\"min_track_length\"],\n",
    "        )\n",
    "        for event in tqdm(graphset)\n",
    "    ]\n",
    "    evaluated_events = pd.concat(evaluated_events)\n",
    "\n",
    "    particles = evaluated_events[evaluated_events[\"is_reconstructable\"]]\n",
    "    reconstructed_particles = particles[particles[\"is_reconstructed\"] & particles[\"is_matchable\"]]\n",
    "    tracks = evaluated_events[evaluated_events[\"is_matchable\"]]\n",
    "    matched_tracks = tracks[tracks[\"is_matched\"]]\n",
    "\n",
    "    n_particles = len(particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "    n_reconstructed_particles = len(reconstructed_particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "\n",
    "    n_tracks = len(tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "    n_matched_tracks = len(matched_tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "\n",
    "    n_dup_reconstructed_particles = len(reconstructed_particles) - n_reconstructed_particles\n",
    "\n",
    "    print(f\"Number of reconstructed particles: {n_reconstructed_particles}\")\n",
    "    print(f\"Number of particles: {n_particles}\")\n",
    "    print(f\"Number of matched tracks: {n_matched_tracks}\")\n",
    "    print(f\"Number of tracks: {n_tracks}\")\n",
    "    print(f\"Number of duplicate reconstructed particles: {n_dup_reconstructed_particles}\")   \n",
    "\n",
    "    # Plot the results across pT and eta\n",
    "    eff = n_reconstructed_particles / n_particles\n",
    "    fake_rate = 1 - (n_matched_tracks / n_tracks)\n",
    "    dup_rate = n_dup_reconstructed_particles / n_reconstructed_particles\n",
    "\n",
    "    logging.info(f\"Efficiency: {eff:.3f}\")\n",
    "    logging.info(f\"Fake rate: {fake_rate:.3f}\")\n",
    "    logging.info(f\"Duplication rate: {dup_rate:.3f}\")\n",
    "    print(f\"Efficiency: {eff:.3f}\")\n",
    "    print(f\"Fake rate: {fake_rate:.3f}\")\n",
    "    print(f\"Duplication rate: {dup_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_result_summary(\n",
    "    n_reconstructed_particles,\n",
    "    n_particles,\n",
    "    n_matched_tracks,\n",
    "    n_tracks,\n",
    "    n_dup_reconstructed_particles,\n",
    "    eff,\n",
    "    fake_rate,\n",
    "    dup_rate,\n",
    "):\n",
    "    summary = f\"Number of reconstructed particles: {n_reconstructed_particles}\\n\"\n",
    "    summary += f\"Number of particles: {n_particles}\\n\"\n",
    "    summary += f\"Number of matched tracks: {n_matched_tracks}\\n\"\n",
    "    summary += f\"Number of tracks: {n_tracks}\\n\"\n",
    "    summary += (\n",
    "        \"Number of duplicate reconstructed particles:\"\n",
    "        f\" {n_dup_reconstructed_particles}\\n\"\n",
    "    )\n",
    "    summary += f\"Efficiency: {eff:.3f}\\n\"\n",
    "    summary += f\"Fake rate: {fake_rate:.3f}\\n\"\n",
    "    summary += f\"Duplication rate: {dup_rate:.3f}\\n\"\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def tracking_efficiency(dataset, config): #plot_config,\n",
    "    \"\"\"\n",
    "    Plot the track efficiency vs. pT of the edge.\n",
    "    \"\"\"\n",
    "    all_y_truth, all_pt = [], []\n",
    "    #dataset = getattr(self, config[\"dataset\"])\n",
    "\n",
    "    evaluated_events = []\n",
    "    for event in tqdm(dataset):\n",
    "        evaluated_events.append(\n",
    "            utils.evaluate_labelled_graph(\n",
    "                event,\n",
    "                matching_fraction=config[\"matching_fraction\"],\n",
    "                matching_style=config[\"matching_style\"],\n",
    "                sel_conf=config[\"target_tracks\"],\n",
    "                min_track_length=config[\"min_track_length\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    evaluated_events = pd.concat(evaluated_events)\n",
    "\n",
    "    particles = evaluated_events[evaluated_events[\"is_reconstructable\"]]\n",
    "    reconstructed_particles = particles[\n",
    "        particles[\"is_reconstructed\"] & particles[\"is_matchable\"]\n",
    "    ]\n",
    "    tracks = evaluated_events[evaluated_events[\"is_matchable\"]]\n",
    "    matched_tracks = tracks[tracks[\"is_matched\"]]\n",
    "\n",
    "    n_particles = len(particles.drop_duplicates(subset=[\"event_id\", \"particle_id\"]))\n",
    "    n_reconstructed_particles = len(\n",
    "        reconstructed_particles.drop_duplicates(subset=[\"event_id\", \"particle_id\"])\n",
    "    )\n",
    "\n",
    "    n_tracks = len(tracks.drop_duplicates(subset=[\"event_id\", \"track_id\"]))\n",
    "    n_matched_tracks = len(\n",
    "        matched_tracks.drop_duplicates(subset=[\"event_id\", \"track_id\"])\n",
    "    )\n",
    "\n",
    "    n_dup_reconstructed_particles = (\n",
    "        len(reconstructed_particles) - n_reconstructed_particles\n",
    "    )\n",
    "\n",
    "    eff = n_reconstructed_particles / n_particles\n",
    "    fake_rate = 1 - (n_matched_tracks / n_tracks)\n",
    "    dup_rate = n_dup_reconstructed_particles / n_reconstructed_particles\n",
    "\n",
    "    result_summary = make_result_summary(\n",
    "        n_reconstructed_particles,\n",
    "        n_particles,\n",
    "        n_matched_tracks,\n",
    "        n_tracks,\n",
    "        n_dup_reconstructed_particles,\n",
    "        eff,\n",
    "        fake_rate,\n",
    "        dup_rate,\n",
    "    )\n",
    "\n",
    "    print(f\"Number of reconstructed particles: {n_reconstructed_particles}\")\n",
    "    print(f\"Number of particles: {n_particles}\")\n",
    "    print(f\"Number of matched tracks: {n_matched_tracks}\")\n",
    "    print(f\"Number of tracks: {n_tracks}\")\n",
    "    print(f\"Number of duplicate reconstructed particles: {n_dup_reconstructed_particles}\")   \n",
    "    print(f\"Efficiency: {eff:.3f}\")\n",
    "    print(f\"Fake rate: {fake_rate:.3f}\")\n",
    "    print(f\"Duplication rate: {dup_rate:.3f}\")\n",
    "\n",
    "    #self.log.info(\"Result Summary :\\n\\n\" + result_summary)\n",
    "\n",
    "    # res_fname = os.path.join(\n",
    "    #     self.hparams[\"stage_dir\"],\n",
    "    #     f\"results_summary_{self.hparams['matching_style']}.txt\",\n",
    "    # )\n",
    "\n",
    "    # with open(res_fname, \"w\") as f:\n",
    "    #     f.write(result_summary)\n",
    "\n",
    "    # First get the list of particles without duplicates\n",
    "    grouped_reco_particles = particles.groupby(\"particle_id\")[\n",
    "        \"is_reconstructed\"\n",
    "    ].any()\n",
    "    # particles[\"is_reconstructed\"] = particles[\"particle_id\"].isin(grouped_reco_particles[grouped_reco_particles].index.values)\n",
    "    particles.loc[\n",
    "        particles[\"particle_id\"].isin(\n",
    "            grouped_reco_particles[grouped_reco_particles].index.values\n",
    "        ),\n",
    "        \"is_reconstructed\",\n",
    "    ] = True\n",
    "    particles = particles.drop_duplicates(subset=[\"particle_id\"])\n",
    "\n",
    "    # Plot the results across pT and eta (if provided in conf file)\n",
    "    #os.makedirs(self.hparams[\"stage_dir\"], exist_ok=True)\n",
    "\n",
    "    # for var, varconf in plot_config[\"variables\"].items():\n",
    "    #     utils.plot_eff(\n",
    "    #         particles,\n",
    "    #         var,\n",
    "    #         varconf,\n",
    "    #         save_path=os.path.join(\n",
    "    #             self.hparams[\"stage_dir\"],\n",
    "    #             f\"track_reconstruction_eff_vs_{var}_{self.hparams['matching_style']}.png\",\n",
    "    #         ),\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reconstructed particles: 1263\n",
      "Number of particles: 2277\n",
      "Number of matched tracks: 29530\n",
      "Number of tracks: 41903\n",
      "Number of duplicate reconstructed particles: 813\n",
      "Efficiency: 0.555\n",
      "Fake rate: 0.295\n",
      "Duplication rate: 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device ='cuda'\n",
    "model_mm = model_mm.to(\"cuda\")\n",
    "model_gnn = model_gnn.to(\"cuda\")\n",
    "graphs = []\n",
    "for batch_idx, (graph, _, truth) in enumerate(model_mm.testset):\n",
    "    print(batch_idx)\n",
    "    batch = model_mm.build_graph(graph.to(device), truth).to(device)\n",
    "    batch.weights = handle_weighting(batch, model_gnn.hparams[\"weighting\"]) #torch.ones_like(batch.y, dtype=torch.float32)\n",
    "    gnn = model_gnn.shared_evaluation(batch,batch_idx)\n",
    "    batch = gnn['batch']\n",
    "    # with torch.no_grad():\n",
    "    #     if device == 'cuda':\n",
    "    #         with torch.cuda.amp.autocast():\n",
    "    #             out = model_gnn(batch)\n",
    "    # batch.scores = torch.sigmoid(out)\n",
    "    #model_gnn.log_metrics(gnn['output'],gnn['all_truth'],gnn['target_truth'],gnn['loss'])\n",
    "    edge_mask = gnn['output'] > 0.8 #model_gnn.hparams['edge_cut'] # score_cut for evaluation\n",
    "    \n",
    "    # Get number of nodes\n",
    "    if hasattr(batch, \"num_nodes\"):\n",
    "        num_nodes = batch.num_nodes\n",
    "    elif hasattr(batch, \"x\"):\n",
    "        num_nodes = batch.x.size(0)\n",
    "    elif hasattr(batch, \"x_x\"):\n",
    "        num_nodes = batch.x_x.size(0)\n",
    "    else:\n",
    "        num_nodes = batch.edge_index.max().item() + 1\n",
    "    # Convert to sparse scipy array\n",
    "    sparse_edges = to_scipy_sparse_matrix(\n",
    "        batch.edge_index[:, edge_mask], num_nodes=num_nodes\n",
    "    )\n",
    "    # Run connected components\n",
    "    candidate_labels = sps.csgraph.connected_components(\n",
    "        sparse_edges, directed=False, return_labels=True\n",
    "    )\n",
    "    batch.labels = torch.from_numpy(candidate_labels[1]).long()\n",
    "    graphs.append(batch.to('cpu'))\n",
    "\n",
    "#evaluate_labelled_graphs(graphs, config_tbe)\n",
    "tracking_efficiency(graphs, config_tbe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
