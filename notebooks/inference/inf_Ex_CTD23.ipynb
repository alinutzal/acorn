{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/global/u2/a/alazar/acorn/acorn/core/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import acorn.core\n",
    "print(acorn.core.__file__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "import yaml\n",
    "from itertools import chain, product, combinations\n",
    "import torch\n",
    "\n",
    "from time import time as tt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from acorn.stages.data_reading import AthenaReader\n",
    "from acorn.core.infer_stage import infer\n",
    "from acorn.core.eval_stage import evaluate\n",
    "\n",
    "from acorn.stages.data_reading.models.trackml_utils import *\n",
    "\n",
    "from acorn.stages.data_reading.data_reading_stage import EventReader\n",
    "from acorn.stages.data_reading.models.trackml_reader import TrackMLReader\n",
    "\n",
    "from acorn.stages.graph_construction.models.metric_learning import MetricLearning\n",
    "from acorn.stages.edge_classifier.models.filter import Filter\n",
    "from acorn.stages.edge_classifier import InteractionGNN\n",
    "from acorn.stages.edge_classifier.edge_classifier_stage import EdgeClassifierStage\n",
    "\n",
    "from acorn.stages.graph_construction.utils import handle_weighting\n",
    "from acorn.stages.graph_construction.models.utils import graph_intersection, build_edges\n",
    "from acorn.stages.graph_construction.utils import *\n",
    "from acorn.stages.graph_construction.models.py_module_map import PyModuleMap\n",
    "from acorn.stages.graph_construction.graph_construction_stage import GraphConstructionStage\n",
    "\n",
    "from acorn.stages.track_building import utils \n",
    "from torch_geometric.utils import to_scipy_sparse_matrix\n",
    "\n",
    "#run = wandb.init(project=model_gnn.hparams[\"project\"], entity='gnnproject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidr = \"../../examples/CTD_2023/data_reader.yaml\"\n",
    "configmm = \"../../examples/CTD_2023/module_map_infer.yaml\"\n",
    "configml = \"../../examples/CTD_2023/metric_learning_infer.yaml\"\n",
    "configml_ev = \"../../examples/CTD_2023/metric_learning_eval.yaml\"\n",
    "configfil = \"../../examples/CTD_2023/filter_infer.yaml\"\n",
    "configfil_ev = \"../../examples/CTD_2023/filter_eval.yaml\"\n",
    "configGnn = \"../../examples/CTD_2023/gnn_infer.yaml\"\n",
    "configGnn_eval = \"../../examples/CTD_2023/gnn_eval.yaml\"\n",
    "configTbi = \"../../examples/CTD_2023/track_building_infer.yaml\"\n",
    "configTbe = \"../../examples/CTD_2023/track_building_eval.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:TrackBuilding:Using log level INFO\n",
      "INFO:TrackBuilding:Using CCandWalk method to reconstruct the tracks\n",
      "Reconstructing tracks for trainset data:   0%|          | 0/80 [00:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NetworkXUnfeasible",
     "evalue": "Graph contains a cycle or graph changed during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNetworkXUnfeasible\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#infer(configml)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#evaluate(configml_ev,dataset='valset')\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#infer(configfil)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#evaluate(configfil_ev,dataset='valset')\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#infer(configGnn)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#evaluate(configGnn_eval,dataset='valset')\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfigTbi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#evaluate(configTbe,None)\u001b[39;00m\n",
      "File \u001b[0;32m/global/u2/a/alazar/acorn/acorn/core/infer_stage.py:71\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(config_file, verbose, checkpoint)\u001b[0m\n\u001b[1;32m     69\u001b[0m     lightning_infer(config, stage_module, checkpoint)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mstage_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/u2/a/alazar/acorn/acorn/stages/track_building/track_building_stage.py:122\u001b[0m, in \u001b[0;36mTrackBuildingStage.infer\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtestset\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(graph_constructor, data_name):\n\u001b[0;32m--> 122\u001b[0m         \u001b[43mgraph_constructor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_tracks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgraph_constructor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_name\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/u2/a/alazar/acorn/acorn/stages/track_building/models/cc_and_walk.py:68\u001b[0m, in \u001b[0;36mCCandWalk.build_tracks\u001b[0;34m(self, dataset, data_name)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[1;32m     66\u001b[0m         dataset, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReconstructing tracks for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m     ):\n\u001b[0;32m---> 68\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_tracks_one_evt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/u2/a/alazar/acorn/acorn/stages/track_building/models/cc_and_walk.py:92\u001b[0m, in \u001b[0;36mCCandWalk._build_tracks_one_evt\u001b[0;34m(self, graph, output_dir)\u001b[0m\n\u001b[1;32m     90\u001b[0m G \u001b[38;5;241m=\u001b[39m cc_and_walk_utils\u001b[38;5;241m.\u001b[39mfilter_graph(graph, score_name, threshold)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# topological sort is really needed only if we run the wrangler afterwards\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[43mcc_and_walk_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopological_sort_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Simple paths from connected components\u001b[39;00m\n\u001b[1;32m     95\u001b[0m all_trks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m cc_and_walk_utils\u001b[38;5;241m.\u001b[39mget_simple_path(G)\n",
      "File \u001b[0;32m/global/u2/a/alazar/acorn/acorn/stages/track_building/models/cc_and_walk_utils.py:65\u001b[0m, in \u001b[0;36mtopological_sort_graph\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m     62\u001b[0m H \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mDiGraph()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Add nodes w/o any features attached\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# maybe this is not needed given line 48?\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[43mH\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_nodes_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopological_sort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# put it after the add nodes\u001b[39;00m\n\u001b[1;32m     68\u001b[0m H\u001b[38;5;241m.\u001b[39madd_edges_from(G\u001b[38;5;241m.\u001b[39medges(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/.conda/envs/acorn/lib/python3.10/site-packages/networkx/classes/digraph.py:530\u001b[0m, in \u001b[0;36mDiGraph.add_nodes_from\u001b[0;34m(self, nodes_for_adding, **attr)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_nodes_from\u001b[39m(\u001b[38;5;28mself\u001b[39m, nodes_for_adding, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mattr):\n\u001b[1;32m    470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Add multiple nodes.\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m    >>> G.add_nodes_from(list(n + 1 for n in G.nodes))\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes_for_adding:\n\u001b[1;32m    531\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m             newnode \u001b[38;5;241m=\u001b[39m n \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node\n",
      "File \u001b[0;32m~/.conda/envs/acorn/lib/python3.10/site-packages/networkx/algorithms/dag.py:309\u001b[0m, in \u001b[0;36mtopological_sort\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;129m@nx\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtopological_sort\u001b[39m(G):\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a generator of nodes in topologically sorted order.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    A topological sort is a nonunique permutation of the nodes of a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m       *Introduction to Algorithms - A Creative Approach.* Addison-Wesley.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mtopological_generations(G):\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m generation\n",
      "File \u001b[0;32m~/.conda/envs/acorn/lib/python3.10/site-packages/networkx/algorithms/dag.py:239\u001b[0m, in \u001b[0;36mtopological_generations\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m this_generation\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indegree_map:\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXUnfeasible(\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph contains a cycle or graph changed during iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m     )\n",
      "\u001b[0;31mNetworkXUnfeasible\u001b[0m: Graph contains a cycle or graph changed during iteration"
     ]
    }
   ],
   "source": [
    "#infer(configml)\n",
    "#evaluate(configml_ev,dataset='valset')\n",
    "#infer(configfil)\n",
    "#evaluate(configfil_ev,dataset='valset')\n",
    "#infer(configGnn)\n",
    "#evaluate(configGnn_eval,dataset='valset')\n",
    "infer(configTbi)\n",
    "#evaluate(configTbe,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/cf/CTD_2023/feature_store/trainset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#model_mm.setup(stage=\"predict\")\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model_mm\u001b[38;5;241m.\u001b[39mload_module_map()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel_mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/scratch/cf/CTD_2023/feature_store/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/u2/a/alazar/acorn/acorn/stages/graph_construction/graph_construction_stage.py:85\u001b[0m, in \u001b[0;36mGraphConstructionStage.load_data\u001b[0;34m(self, input_dir)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03mLoad in the data for training, validation and testing.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_name, data_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m     83\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrainset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtestset\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_split\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     84\u001b[0m ):\n\u001b[0;32m---> 85\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_pyg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_pyg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_csv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, data_name, dataset)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m training events,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m validation events and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtestset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m testing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m events\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m )\n",
      "File \u001b[0;32m/global/u2/a/alazar/acorn/acorn/stages/graph_construction/graph_construction_stage.py:258\u001b[0m, in \u001b[0;36mEventDataset.__init__\u001b[0;34m(self, input_dir, data_name, num_events, use_pyg, use_csv, preload, hparams, transform, pre_transform, pre_filter, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_pyg \u001b[38;5;241m=\u001b[39m use_pyg\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_csv \u001b[38;5;241m=\u001b[39m use_csv\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevt_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_evt_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/u2/a/alazar/acorn/acorn/stages/graph_construction/graph_construction_stage.py:320\u001b[0m, in \u001b[0;36mEventDataset.find_evt_ids\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_evt_ids\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    316\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03m    Returns a list of all event ids, which are the numbers in filenames that end in .csv and .pyg\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     all_files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     all_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m all_files \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pyg\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m    322\u001b[0m     all_event_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;28mlist\u001b[39m({re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[0-9]+\u001b[39m\u001b[38;5;124m\"\u001b[39m, file)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m all_files})\n\u001b[1;32m    324\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/scratch/cf/CTD_2023/feature_store/trainset'"
     ]
    }
   ],
   "source": [
    "with open(\"../../examples/CTD_2023/module_map_infer.yaml\", \"r\") as f:\n",
    "    config_mm = yaml.load(f, Loader=yaml.FullLoader)\n",
    "model_mm = PyModuleMap(config_mm)\n",
    "#model_mm.setup(stage=\"predict\")\n",
    "model_mm.load_module_map()\n",
    "model_mm.load_data(\"/scratch/cf/CTD_2023/feature_store/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [particle_id] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [nhits] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [primary] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [pdgId] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [ghost] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [shared] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [module_id] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [region] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [hit_id] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/utils/loading_utils.py:78: UserWarning: OPTIONAL feature [pt] not found in data\n",
      "  warnings.warn(f\"OPTIONAL feature [{feature}] not found in data\")\n",
      "/global/homes/a/alazar/acorn_new/acorn/stages/edge_classifier/edge_classifier_stage.py:96: UserWarning: Failed to define figures of merit, due to logger unavailable\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining figures of merit\n",
      "/scratch/cf/Example_1/gnn/artifacts\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../examples/CTD_2023/gnn_train.yaml\", \"r\") as f:\n",
    "    config_gnn = yaml.load(f, Loader=yaml.FullLoader)\n",
    "model_gnn = InteractionGNN(config_gnn)\n",
    "model_gnn.setup('predict')\n",
    "model_gnn = InteractionGNN.load_from_checkpoint(config_gnn['stage_dir']+'artifacts/best-v3.ckpt')  \n",
    "                                                #best-4l0jlwuh-val_loss=0.085163-epoch=77.ckpt')\n",
    "#dataloaders_gnn = model_gnn.predict_dataloader()\n",
    "\n",
    "config_tbe = yaml.safe_load(open(\"../../examples/CTD_2023/track_building_eval.yaml\", \"r\"))\n",
    "print(config_gnn['stage_dir']+'artifacts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labelled_graphs(graphset, config):\n",
    "    all_y_truth, all_pt  = [], []\n",
    "    evaluated_events = [\n",
    "        utils.evaluate_labelled_graph(\n",
    "            event,\n",
    "            sel_conf=config[\"target_tracks\"],\n",
    "            matching_fraction=config[\"matching_fraction\"],\n",
    "            matching_style=config[\"matching_style\"],\n",
    "            min_track_length=config[\"min_track_length\"],\n",
    "        )\n",
    "        for event in tqdm(graphset)\n",
    "    ]\n",
    "    evaluated_events = pd.concat(evaluated_events)\n",
    "\n",
    "    particles = evaluated_events[evaluated_events[\"is_reconstructable\"]]\n",
    "    reconstructed_particles = particles[particles[\"is_reconstructed\"] & particles[\"is_matchable\"]]\n",
    "    tracks = evaluated_events[evaluated_events[\"is_matchable\"]]\n",
    "    matched_tracks = tracks[tracks[\"is_matched\"]]\n",
    "\n",
    "    n_particles = len(particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "    n_reconstructed_particles = len(reconstructed_particles.drop_duplicates(subset=['event_id', 'particle_id']))\n",
    "\n",
    "    n_tracks = len(tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "    n_matched_tracks = len(matched_tracks.drop_duplicates(subset=['event_id', 'track_id']))\n",
    "\n",
    "    n_dup_reconstructed_particles = len(reconstructed_particles) - n_reconstructed_particles\n",
    "\n",
    "    print(f\"Number of reconstructed particles: {n_reconstructed_particles}\")\n",
    "    print(f\"Number of particles: {n_particles}\")\n",
    "    print(f\"Number of matched tracks: {n_matched_tracks}\")\n",
    "    print(f\"Number of tracks: {n_tracks}\")\n",
    "    print(f\"Number of duplicate reconstructed particles: {n_dup_reconstructed_particles}\")   \n",
    "\n",
    "    # Plot the results across pT and eta\n",
    "    eff = n_reconstructed_particles / n_particles\n",
    "    fake_rate = 1 - (n_matched_tracks / n_tracks)\n",
    "    dup_rate = n_dup_reconstructed_particles / n_reconstructed_particles\n",
    "\n",
    "    logging.info(f\"Efficiency: {eff:.3f}\")\n",
    "    logging.info(f\"Fake rate: {fake_rate:.3f}\")\n",
    "    logging.info(f\"Duplication rate: {dup_rate:.3f}\")\n",
    "    print(f\"Efficiency: {eff:.3f}\")\n",
    "    print(f\"Fake rate: {fake_rate:.3f}\")\n",
    "    print(f\"Duplication rate: {dup_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_result_summary(\n",
    "    n_reconstructed_particles,\n",
    "    n_particles,\n",
    "    n_matched_tracks,\n",
    "    n_tracks,\n",
    "    n_dup_reconstructed_particles,\n",
    "    eff,\n",
    "    fake_rate,\n",
    "    dup_rate,\n",
    "):\n",
    "    summary = f\"Number of reconstructed particles: {n_reconstructed_particles}\\n\"\n",
    "    summary += f\"Number of particles: {n_particles}\\n\"\n",
    "    summary += f\"Number of matched tracks: {n_matched_tracks}\\n\"\n",
    "    summary += f\"Number of tracks: {n_tracks}\\n\"\n",
    "    summary += (\n",
    "        \"Number of duplicate reconstructed particles:\"\n",
    "        f\" {n_dup_reconstructed_particles}\\n\"\n",
    "    )\n",
    "    summary += f\"Efficiency: {eff:.3f}\\n\"\n",
    "    summary += f\"Fake rate: {fake_rate:.3f}\\n\"\n",
    "    summary += f\"Duplication rate: {dup_rate:.3f}\\n\"\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def tracking_efficiency(dataset, config): #plot_config,\n",
    "    \"\"\"\n",
    "    Plot the track efficiency vs. pT of the edge.\n",
    "    \"\"\"\n",
    "    all_y_truth, all_pt = [], []\n",
    "    #dataset = getattr(self, config[\"dataset\"])\n",
    "\n",
    "    evaluated_events = []\n",
    "    for event in tqdm(dataset):\n",
    "        evaluated_events.append(\n",
    "            utils.evaluate_labelled_graph(\n",
    "                event,\n",
    "                matching_fraction=config[\"matching_fraction\"],\n",
    "                matching_style=config[\"matching_style\"],\n",
    "                sel_conf=config[\"target_tracks\"],\n",
    "                min_track_length=config[\"min_track_length\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    evaluated_events = pd.concat(evaluated_events)\n",
    "\n",
    "    particles = evaluated_events[evaluated_events[\"is_reconstructable\"]]\n",
    "    reconstructed_particles = particles[\n",
    "        particles[\"is_reconstructed\"] & particles[\"is_matchable\"]\n",
    "    ]\n",
    "    tracks = evaluated_events[evaluated_events[\"is_matchable\"]]\n",
    "    matched_tracks = tracks[tracks[\"is_matched\"]]\n",
    "\n",
    "    n_particles = len(particles.drop_duplicates(subset=[\"event_id\", \"particle_id\"]))\n",
    "    n_reconstructed_particles = len(\n",
    "        reconstructed_particles.drop_duplicates(subset=[\"event_id\", \"particle_id\"])\n",
    "    )\n",
    "\n",
    "    n_tracks = len(tracks.drop_duplicates(subset=[\"event_id\", \"track_id\"]))\n",
    "    n_matched_tracks = len(\n",
    "        matched_tracks.drop_duplicates(subset=[\"event_id\", \"track_id\"])\n",
    "    )\n",
    "\n",
    "    n_dup_reconstructed_particles = (\n",
    "        len(reconstructed_particles) - n_reconstructed_particles\n",
    "    )\n",
    "\n",
    "    eff = n_reconstructed_particles / n_particles\n",
    "    fake_rate = 1 - (n_matched_tracks / n_tracks)\n",
    "    dup_rate = n_dup_reconstructed_particles / n_reconstructed_particles\n",
    "\n",
    "    result_summary = make_result_summary(\n",
    "        n_reconstructed_particles,\n",
    "        n_particles,\n",
    "        n_matched_tracks,\n",
    "        n_tracks,\n",
    "        n_dup_reconstructed_particles,\n",
    "        eff,\n",
    "        fake_rate,\n",
    "        dup_rate,\n",
    "    )\n",
    "\n",
    "    print(f\"Number of reconstructed particles: {n_reconstructed_particles}\")\n",
    "    print(f\"Number of particles: {n_particles}\")\n",
    "    print(f\"Number of matched tracks: {n_matched_tracks}\")\n",
    "    print(f\"Number of tracks: {n_tracks}\")\n",
    "    print(f\"Number of duplicate reconstructed particles: {n_dup_reconstructed_particles}\")   \n",
    "    print(f\"Efficiency: {eff:.3f}\")\n",
    "    print(f\"Fake rate: {fake_rate:.3f}\")\n",
    "    print(f\"Duplication rate: {dup_rate:.3f}\")\n",
    "\n",
    "    #self.log.info(\"Result Summary :\\n\\n\" + result_summary)\n",
    "\n",
    "    # res_fname = os.path.join(\n",
    "    #     self.hparams[\"stage_dir\"],\n",
    "    #     f\"results_summary_{self.hparams['matching_style']}.txt\",\n",
    "    # )\n",
    "\n",
    "    # with open(res_fname, \"w\") as f:\n",
    "    #     f.write(result_summary)\n",
    "\n",
    "    # First get the list of particles without duplicates\n",
    "    grouped_reco_particles = particles.groupby(\"particle_id\")[\n",
    "        \"is_reconstructed\"\n",
    "    ].any()\n",
    "    # particles[\"is_reconstructed\"] = particles[\"particle_id\"].isin(grouped_reco_particles[grouped_reco_particles].index.values)\n",
    "    particles.loc[\n",
    "        particles[\"particle_id\"].isin(\n",
    "            grouped_reco_particles[grouped_reco_particles].index.values\n",
    "        ),\n",
    "        \"is_reconstructed\",\n",
    "    ] = True\n",
    "    particles = particles.drop_duplicates(subset=[\"particle_id\"])\n",
    "\n",
    "    # Plot the results across pT and eta (if provided in conf file)\n",
    "    #os.makedirs(self.hparams[\"stage_dir\"], exist_ok=True)\n",
    "\n",
    "    # for var, varconf in plot_config[\"variables\"].items():\n",
    "    #     utils.plot_eff(\n",
    "    #         particles,\n",
    "    #         var,\n",
    "    #         varconf,\n",
    "    #         save_path=os.path.join(\n",
    "    #             self.hparams[\"stage_dir\"],\n",
    "    #             f\"track_reconstruction_eff_vs_{var}_{self.hparams['matching_style']}.png\",\n",
    "    #         ),\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reconstructed particles: 1263\n",
      "Number of particles: 2277\n",
      "Number of matched tracks: 29530\n",
      "Number of tracks: 41903\n",
      "Number of duplicate reconstructed particles: 813\n",
      "Efficiency: 0.555\n",
      "Fake rate: 0.295\n",
      "Duplication rate: 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device ='cuda'\n",
    "model_mm = model_mm.to(\"cuda\")\n",
    "model_gnn = model_gnn.to(\"cuda\")\n",
    "graphs = []\n",
    "for batch_idx, (graph, _, truth) in enumerate(model_mm.testset):\n",
    "    print(batch_idx)\n",
    "    batch = model_mm.build_graph(graph.to(device), truth).to(device)\n",
    "    batch.weights = handle_weighting(batch, model_gnn.hparams[\"weighting\"]) #torch.ones_like(batch.y, dtype=torch.float32)\n",
    "    gnn = model_gnn.shared_evaluation(batch,batch_idx)\n",
    "    batch = gnn['batch']\n",
    "    # with torch.no_grad():\n",
    "    #     if device == 'cuda':\n",
    "    #         with torch.cuda.amp.autocast():\n",
    "    #             out = model_gnn(batch)\n",
    "    # batch.scores = torch.sigmoid(out)\n",
    "    #model_gnn.log_metrics(gnn['output'],gnn['all_truth'],gnn['target_truth'],gnn['loss'])\n",
    "    edge_mask = gnn['output'] > 0.8 #model_gnn.hparams['edge_cut'] # score_cut for evaluation\n",
    "    \n",
    "    # Get number of nodes\n",
    "    if hasattr(batch, \"num_nodes\"):\n",
    "        num_nodes = batch.num_nodes\n",
    "    elif hasattr(batch, \"x\"):\n",
    "        num_nodes = batch.x.size(0)\n",
    "    elif hasattr(batch, \"x_x\"):\n",
    "        num_nodes = batch.x_x.size(0)\n",
    "    else:\n",
    "        num_nodes = batch.edge_index.max().item() + 1\n",
    "    # Convert to sparse scipy array\n",
    "    sparse_edges = to_scipy_sparse_matrix(\n",
    "        batch.edge_index[:, edge_mask], num_nodes=num_nodes\n",
    "    )\n",
    "    # Run connected components\n",
    "    candidate_labels = sps.csgraph.connected_components(\n",
    "        sparse_edges, directed=False, return_labels=True\n",
    "    )\n",
    "    batch.labels = torch.from_numpy(candidate_labels[1]).long()\n",
    "    graphs.append(batch.to('cpu'))\n",
    "\n",
    "#evaluate_labelled_graphs(graphs, config_tbe)\n",
    "tracking_efficiency(graphs, config_tbe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn4itk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
